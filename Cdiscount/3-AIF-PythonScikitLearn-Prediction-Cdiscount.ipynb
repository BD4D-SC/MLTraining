{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importation des librairies utilisées\n",
    "import unicodedata \n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import collections\n",
    "import itertools\n",
    "import csv\n",
    "import warnings\n",
    "import pickle\n",
    "import scipy\n",
    "\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = pd.read_csv(\"data/cdiscount_train_subset.csv\").fillna(\"\")[\"Categorie1\"]\n",
    "Y_test = pd.read_csv(\"data/cdiscount_test.csv\").fillna(\"\")[\"Categorie1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_hash : None, vectorizer : count\n",
      "{'typeW2V': None, 'nb_hash': None, 'vectorizer': 'count', 'learning_time': 65.79484820365906, 'predict_time': 0.20164084434509277, 'score_train': 0.9799368421052631, 'score_valid': 0.9066}\n",
      "nb_hash : 300, vectorizer : count\n",
      "{'typeW2V': None, 'nb_hash': 300, 'vectorizer': 'count', 'learning_time': 57.50672173500061, 'predict_time': 0.21946358680725098, 'score_train': 0.7234, 'score_valid': 0.7076}\n",
      "nb_hash : 10000, vectorizer : count\n",
      "{'typeW2V': None, 'nb_hash': 10000, 'vectorizer': 'count', 'learning_time': 69.89495491981506, 'predict_time': 0.18376874923706055, 'score_train': 0.9694, 'score_valid': 0.888}\n",
      "nb_hash : None, vectorizer : tfidf\n",
      "{'typeW2V': None, 'nb_hash': None, 'vectorizer': 'tfidf', 'learning_time': 35.86675786972046, 'predict_time': 0.20187044143676758, 'score_train': 0.9134736842105263, 'score_valid': 0.8824}\n",
      "nb_hash : 300, vectorizer : tfidf\n",
      "{'typeW2V': None, 'nb_hash': 300, 'vectorizer': 'tfidf', 'learning_time': 36.75836682319641, 'predict_time': 0.15125036239624023, 'score_train': 0.7169052631578947, 'score_valid': 0.7004}\n",
      "nb_hash : 10000, vectorizer : tfidf\n",
      "{'typeW2V': None, 'nb_hash': 10000, 'vectorizer': 'tfidf', 'learning_time': 32.383073568344116, 'predict_time': 0.18765640258789062, 'score_train': 0.9019894736842106, 'score_valid': 0.8648}\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"data/features\"\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "metadata_list_lr = []\n",
    "\n",
    "parameters = [[None, \"count\"],\n",
    "              [300, \"count\"],\n",
    "              [10000, \"count\"],\n",
    "              [None, \"tfidf\"],\n",
    "              [300, \"tfidf\"],\n",
    "              [10000, \"tfidf\"],]\n",
    "\n",
    "for nb_hash, vectorizer in parameters:\n",
    "    print(\"nb_hash : \" + str(nb_hash) + \", vectorizer : \" + str(vectorizer))\n",
    "    X_train = sparse.load_npz(DATA_DIR +\"/vec_train_nb_hash_\" + str(nb_hash) + \"_vectorizer_\" + str(vectorizer)+\".npz\")\n",
    "    X_test = sparse.load_npz(DATA_DIR +\"/vec_test_nb_hash_\" + str(nb_hash) + \"_vectorizer_\" + str(vectorizer)+\".npz\")\n",
    "    ts = time.time()\n",
    "    cla = LogisticRegression()\n",
    "    cla.fit(X_train,Y_train.values)\n",
    "    te=time.time()\n",
    "    t_learning = te-ts\n",
    "    ts = time.time()\n",
    "    score_train=cla.score(X_train,Y_train)\n",
    "    score_test=cla.score(X_test,Y_test)\n",
    "    te=time.time()\n",
    "    t_predict = te-ts\n",
    "    metadata = {\"typeW2V\": None, \"nb_hash\": nb_hash, \"vectorizer\":vectorizer , \"learning_time\" : t_learning, \"predict_time\":t_predict, \"score_train\": score_train, \"score_test\": score_test}\n",
    "    print(metadata)\n",
    "    metadata_list_lr.append(metadata)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word2Vec :CBOW\n",
      "{'typeW2V': 'CBOW', 'nb_hash': None, 'vectorizer': 'word2vec', 'learning_time': 744.2787802219391, 'predict_time': 0.21297192573547363, 'score_train': 0.8030842105263158, 'score_valid': 0.8026}\n",
      "Word2Vec :skip-gram\n",
      "{'typeW2V': 'skip-gram', 'nb_hash': None, 'vectorizer': 'word2vec', 'learning_time': 364.36927318573, 'predict_time': 0.21204614639282227, 'score_train': 0.8490736842105263, 'score_valid': 0.843}\n",
      "Word2Vec :online\n",
      "{'typeW2V': 'online', 'nb_hash': None, 'vectorizer': 'word2vec', 'learning_time': 757.8160936832428, 'predict_time': 0.18524646759033203, 'score_train': 0.7748, 'score_valid': 0.7656}\n"
     ]
    }
   ],
   "source": [
    "# Temps d'exécution un peu long....\n",
    "\n",
    "for model_name in [\"CBOW\",\"skip-gram\", \"online\"]:\n",
    "    print(\"Word2Vec :\" + model_name)\n",
    "\n",
    "    X_train = np.load(DATA_DIR +\"/embedded_train_nb_hash_\" + model_name+\".npy\")\n",
    "    X_test = np.load(DATA_DIR +\"/embedded_test_nb_hash_\" + model_name+\".npy\")\n",
    "    \n",
    "    ts = time.time()\n",
    "    cla = LogisticRegression()\n",
    "    cla.fit(X_train,Y_train.values)\n",
    "    te=time.time()\n",
    "    t_learning = te-ts\n",
    "    ts = time.time()\n",
    "    score_train=cla.score(X_train,Y_train)\n",
    "    score_test=cla.score(X_test,Y_test)\n",
    "    te=time.time()\n",
    "    t_predict = te-ts\n",
    "    metadata = {\"typeW2V\": model_name ,\"nb_hash\": None, \"vectorizer\":\"word2vec\" ,\"learning_time\" : t_learning, \"predict_time\":t_predict, \"score_train\": score_train, \"score_test\": score_test}\n",
    "    print(metadata)\n",
    "    metadata_list_lr.append(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95000, 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\"data/features/embedded_train_nb_hash_CBOW.npy\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_time</th>\n",
       "      <th>nb_hash</th>\n",
       "      <th>predict_time</th>\n",
       "      <th>score_train</th>\n",
       "      <th>score_valid</th>\n",
       "      <th>typeW2V</th>\n",
       "      <th>vectorizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65.794848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201641</td>\n",
       "      <td>0.979937</td>\n",
       "      <td>0.9066</td>\n",
       "      <td>None</td>\n",
       "      <td>count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57.506722</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.219464</td>\n",
       "      <td>0.723400</td>\n",
       "      <td>0.7076</td>\n",
       "      <td>None</td>\n",
       "      <td>count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.894955</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.183769</td>\n",
       "      <td>0.969400</td>\n",
       "      <td>0.8880</td>\n",
       "      <td>None</td>\n",
       "      <td>count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.866758</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201870</td>\n",
       "      <td>0.913474</td>\n",
       "      <td>0.8824</td>\n",
       "      <td>None</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.758367</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.151250</td>\n",
       "      <td>0.716905</td>\n",
       "      <td>0.7004</td>\n",
       "      <td>None</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32.383074</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.187656</td>\n",
       "      <td>0.901989</td>\n",
       "      <td>0.8648</td>\n",
       "      <td>None</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>744.278780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.212972</td>\n",
       "      <td>0.803084</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>CBOW</td>\n",
       "      <td>word2vec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>364.369273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.212046</td>\n",
       "      <td>0.849074</td>\n",
       "      <td>0.8430</td>\n",
       "      <td>skip-gram</td>\n",
       "      <td>word2vec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>757.816094</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.185246</td>\n",
       "      <td>0.774800</td>\n",
       "      <td>0.7656</td>\n",
       "      <td>online</td>\n",
       "      <td>word2vec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_time  nb_hash  predict_time  score_train  score_valid    typeW2V  \\\n",
       "0      65.794848      NaN      0.201641     0.979937       0.9066       None   \n",
       "1      57.506722    300.0      0.219464     0.723400       0.7076       None   \n",
       "2      69.894955  10000.0      0.183769     0.969400       0.8880       None   \n",
       "3      35.866758      NaN      0.201870     0.913474       0.8824       None   \n",
       "4      36.758367    300.0      0.151250     0.716905       0.7004       None   \n",
       "5      32.383074  10000.0      0.187656     0.901989       0.8648       None   \n",
       "6     744.278780      NaN      0.212972     0.803084       0.8026       CBOW   \n",
       "7     364.369273      NaN      0.212046     0.849074       0.8430  skip-gram   \n",
       "8     757.816094      NaN      0.185246     0.774800       0.7656     online   \n",
       "\n",
       "  vectorizer  \n",
       "0      count  \n",
       "1      count  \n",
       "2      count  \n",
       "3      tfidf  \n",
       "4      tfidf  \n",
       "5      tfidf  \n",
       "6   word2vec  \n",
       "7   word2vec  \n",
       "8   word2vec  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(metadata_list_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_hash : None, vectorizer : count\n",
      "{'typeW2V': None, 'nb_hash': None, 'vectorizer': 'count', 'learning_time': 385.3659739494324, 'predict_time': 11.882237911224365, 'score_train': 0.9986421052631579, 'score_valid': 0.8558}\n",
      "nb_hash : 300, vectorizer : count\n",
      "{'typeW2V': None, 'nb_hash': 300, 'vectorizer': 'count', 'learning_time': 250.5232491493225, 'predict_time': 7.298325300216675, 'score_train': 0.9983578947368421, 'score_valid': 0.7726}\n",
      "nb_hash : 10000, vectorizer : count\n",
      "{'typeW2V': None, 'nb_hash': 10000, 'vectorizer': 'count', 'learning_time': 201.2321469783783, 'predict_time': 13.452975511550903, 'score_train': 0.9986421052631579, 'score_valid': 0.8484}\n",
      "nb_hash : None, vectorizer : tfidf\n",
      "{'typeW2V': None, 'nb_hash': None, 'vectorizer': 'tfidf', 'learning_time': 292.9067871570587, 'predict_time': 10.577593326568604, 'score_train': 0.9986421052631579, 'score_valid': 0.8522}\n",
      "nb_hash : 300, vectorizer : tfidf\n",
      "{'typeW2V': None, 'nb_hash': 300, 'vectorizer': 'tfidf', 'learning_time': 257.513507604599, 'predict_time': 7.3741772174835205, 'score_train': 0.9983578947368421, 'score_valid': 0.7546}\n",
      "nb_hash : 10000, vectorizer : tfidf\n",
      "{'typeW2V': None, 'nb_hash': 10000, 'vectorizer': 'tfidf', 'learning_time': 180.08447551727295, 'predict_time': 12.56999135017395, 'score_train': 0.9986315789473684, 'score_valid': 0.8324}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "metadata_list_rf = []\n",
    "\n",
    "parameters = [[None, \"count\"],\n",
    "              [300, \"count\"],\n",
    "              [10000, \"count\"],\n",
    "              [None, \"tfidf\"],\n",
    "              [300, \"tfidf\"],\n",
    "              [10000, \"tfidf\"],]\n",
    "\n",
    "for nb_hash, vectorizer in parameters:\n",
    "    print(\"nb_hash : \" + str(nb_hash) + \", vectorizer : \" + str(vectorizer))\n",
    "    X_train = sparse.load_npz(DATA_DIR +\"/vec_train_nb_hash_\" + str(nb_hash) + \"_vectorizer_\" + str(vectorizer)+\".npz\")\n",
    "    X_test = sparse.load_npz(DATA_DIR +\"/vec_test_nb_hash_\" + str(nb_hash) + \"_vectorizer_\" + str(vectorizer)+\".npz\")\n",
    "    ts = time.time()\n",
    "    cla = RandomForestClassifier(n_estimators=100)\n",
    "    cla.fit(X_train,Y_train.values)\n",
    "    te=time.time()\n",
    "    t_learning = te-ts\n",
    "    ts = time.time()\n",
    "    score_train=cla.score(X_train,Y_train)\n",
    "    score_test=cla.score(X_test,Y_test)\n",
    "    te=time.time()\n",
    "    t_predict = te-ts\n",
    "    metadata = {\"typeW2V\": None, \"nb_hash\": nb_hash, \"vectorizer\":vectorizer , \"learning_time\" : t_learning, \"predict_time\":t_predict, \"score_train\": score_train, \"score_test\": score_test}\n",
    "    print(metadata)\n",
    "    metadata_list_rf.append(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word2Vec :CBOW\n",
      "{'typeW2V': 'CBOW', 'nb_hash': None, 'vectorizer': 'word2vec', 'learning_time': 330.7274811267853, 'predict_time': 5.565982818603516, 'score_train': 0.9986421052631579, 'score_valid': 0.8324}\n",
      "Word2Vec :skip-gram\n",
      "{'typeW2V': 'skip-gram', 'nb_hash': None, 'vectorizer': 'word2vec', 'learning_time': 329.91056537628174, 'predict_time': 5.531147241592407, 'score_train': 0.9986421052631579, 'score_valid': 0.8536}\n",
      "Word2Vec :online\n",
      "{'typeW2V': 'online', 'nb_hash': None, 'vectorizer': 'word2vec', 'learning_time': 287.7881805896759, 'predict_time': 6.184351205825806, 'score_train': 0.9911368421052632, 'score_valid': 0.7408}\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "\n",
    "for model_name in [\"CBOW\",\"skip-gram\", \"online\"]:\n",
    "    print(\"Word2Vec :\" + model_name)\n",
    "\n",
    "    X_train = np.load(DATA_DIR +\"/embedded_train_nb_hash_\" + model_name+\".npy\")\n",
    "    X_test = np.load(DATA_DIR +\"/embedded_test_nb_hash_\" + model_name+\".npy\")\n",
    "    \n",
    "    ts = time.time()\n",
    "    cla = RandomForestClassifier(n_estimators=100)\n",
    "    cla.fit(X_train,Y_train.values)\n",
    "    te=time.time()\n",
    "    t_learning = te-ts\n",
    "    ts = time.time()\n",
    "    score_train=cla.score(X_train,Y_train)\n",
    "    score_test=cla.score(X_test,Y_test)\n",
    "    te=time.time()\n",
    "    t_predict = te-ts\n",
    "    metadata = {\"typeW2V\": model_name ,\"nb_hash\": None, \"vectorizer\":\"word2vec\" ,\"learning_time\" : t_learning, \"predict_time\":t_predict, \"score_train\": score_train, \"score_test\": score_test}\n",
    "    print(metadata)\n",
    "    metadata_list_rf.append(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_time</th>\n",
       "      <th>nb_hash</th>\n",
       "      <th>predict_time</th>\n",
       "      <th>score_train</th>\n",
       "      <th>score_valid</th>\n",
       "      <th>typeW2V</th>\n",
       "      <th>vectorizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65.794848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201641</td>\n",
       "      <td>0.979937</td>\n",
       "      <td>0.9066</td>\n",
       "      <td>None</td>\n",
       "      <td>count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57.506722</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.219464</td>\n",
       "      <td>0.723400</td>\n",
       "      <td>0.7076</td>\n",
       "      <td>None</td>\n",
       "      <td>count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.894955</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.183769</td>\n",
       "      <td>0.969400</td>\n",
       "      <td>0.8880</td>\n",
       "      <td>None</td>\n",
       "      <td>count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.866758</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201870</td>\n",
       "      <td>0.913474</td>\n",
       "      <td>0.8824</td>\n",
       "      <td>None</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.758367</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.151250</td>\n",
       "      <td>0.716905</td>\n",
       "      <td>0.7004</td>\n",
       "      <td>None</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32.383074</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.187656</td>\n",
       "      <td>0.901989</td>\n",
       "      <td>0.8648</td>\n",
       "      <td>None</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>744.278780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.212972</td>\n",
       "      <td>0.803084</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>CBOW</td>\n",
       "      <td>word2vec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>364.369273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.212046</td>\n",
       "      <td>0.849074</td>\n",
       "      <td>0.8430</td>\n",
       "      <td>skip-gram</td>\n",
       "      <td>word2vec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>757.816094</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.185246</td>\n",
       "      <td>0.774800</td>\n",
       "      <td>0.7656</td>\n",
       "      <td>online</td>\n",
       "      <td>word2vec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_time  nb_hash  predict_time  score_train  score_valid    typeW2V  \\\n",
       "0      65.794848      NaN      0.201641     0.979937       0.9066       None   \n",
       "1      57.506722    300.0      0.219464     0.723400       0.7076       None   \n",
       "2      69.894955  10000.0      0.183769     0.969400       0.8880       None   \n",
       "3      35.866758      NaN      0.201870     0.913474       0.8824       None   \n",
       "4      36.758367    300.0      0.151250     0.716905       0.7004       None   \n",
       "5      32.383074  10000.0      0.187656     0.901989       0.8648       None   \n",
       "6     744.278780      NaN      0.212972     0.803084       0.8026       CBOW   \n",
       "7     364.369273      NaN      0.212046     0.849074       0.8430  skip-gram   \n",
       "8     757.816094      NaN      0.185246     0.774800       0.7656     online   \n",
       "\n",
       "  vectorizer  \n",
       "0      count  \n",
       "1      count  \n",
       "2      count  \n",
       "3      tfidf  \n",
       "4      tfidf  \n",
       "5      tfidf  \n",
       "6   word2vec  \n",
       "7   word2vec  \n",
       "8   word2vec  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(metadata_list_lr)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
