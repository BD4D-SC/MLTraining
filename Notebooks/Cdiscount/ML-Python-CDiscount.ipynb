{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 250px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>\n",
    "\n",
    "# [Tutoriel d'apprentissage automatique](https://github.com/wikistat/MLTraining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *NLP* et Catégorisation de Produits en <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 120px; display: inline\" alt=\"R\"/></a> avec <a href=\"http://scikit-learn.org/stable/#\"><img src=\"http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png\" style=\"max-width: 100px; display: inline\" alt=\"scikit-learn\"/></a> et <a href=\"https://radimrehurek.com/gensim/index.html\" ><img src=\"https://radimrehurek.com/gensim/_static/images/gensim.png\" style=\"max-width: 100px; display: inline\" alt=\"gensim\"/></a> \n",
    "\n",
    "#### Résumé\n",
    "Le principal objectif est d'illuster sur des vraies données les processus d'analyse de données textuelles ou de langage naturel (NLP). Plusieurs étapes sont considérées et exécutées dans l'environnement Pyhton avec le slibrairies spécialisées. Nesttoyage des textes, éliminaiton des mots inutiles, racinisation ainsi que la comparaison de différentes stratgies de vectorisation (comptage, TF-IDF, word2vect). A la suite de quoi l'onjectif de classificaiton en catégorie est atteint en comparant deux méthodes: régresison logistique et forêts aléatoires.\n",
    "\n",
    "\n",
    "## 1 Introduction\n",
    "\n",
    "Il s'agit d'une version simplifiée du concours proposé par CDiscount et paru sur le site [datascience.net](https://www.datascience.net/fr/challenge) puis Kaggle. Les données d'apprentissage sont accessibles sur demande auprès de CDiscount. Les solutions de l'échantillon test du concours ne sont pas et ne seront pas rendues publiques. Un échantillon test est donc construit pour l'usage de ce tutoriel.  L'objectif est de prévoir la catégorie d'un produit à partir de son descriptif. Seule la catégorie principale (1er niveau) est prédite au lieu des trois niveaux demandés dans le concours. L'objectif est d'illustrer sur un exemple complexe le prétraitement de données textuelles ou de langave naturel (*Natural Language Process - NLP*). La stratégie de sous ou sur échantillonnage des catégories qui permet d'améliorer la prévision n'a pas été mise en oeuvre.\n",
    "* L'exemple est présenté sur un échantillon réduit d'un million de produits au lieu des 15M initiaux\n",
    "* L'échantillon réduit peut encore l'être puis séparé en 2 parties: apprentissage et validation. \n",
    "* Les données textuelles sont  nettoyées, racinisées, vectorisées avant modélisation.\n",
    "* Deux modélisations sont estimées: logistique, forêts aléatoires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\">Épisode 1</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Prise en charge des données\n",
    "### 2.1 Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Importation des librairies utilisées\n",
    "import unicodedata \n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import collections\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set_style(\"whitegrid\")\n",
    "\n",
    "import sklearn.model_selection as scv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attetnion** Si vous utilisez la librairie `nltk` pour la première fois dans votre environnement, il est nécessaire d'exécuter la commande suivante. Cette commande permet de télécharger de nombreux corpus de texte, mais également des informations grammaticales sur différentes langues; information notamment nécessaire à l'étape de racinisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Lecture des données\n",
    "\n",
    "* `cdiscount_train`: 1.000.000 de lignes\n",
    "* `cdisount_test`: 50.000 lignes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition d'une fonction permettant de lire le fichier d'apprentissage et de créer aléatoirement deux *data frames  `Pandas`, un pour l'apprentissage, l'autre pour le test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def split_dataset(input_path, nb_line, tauxTest):\n",
    "    data_all = pd.read_csv(input_path,sep=\",\", nrows=nb_line)\n",
    "    data_all = data_all.fillna(\"\")\n",
    "    data_train, data_test = scv.train_test_split(data_all, test_size = tauxTest)\n",
    "    time_end = time.time()\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La taille du fichier est limité à `nb_line=10 000` pour réduire les temps de calcul à venir. Ce nombre peut être augmenté afin de tester une amélioraiton des performances mais au prix du temps de calcul. Cette analyse systématique de l'impact de la taille de l'échantillon est développé par [Besse et al. (2017)](https://hal.archives-ouvertes.fr/hal-01350099v3) qui opèrent de plus une comparaison des performances entre R, Python et Spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"data/cdiscount_train.csv.zip\"\n",
    "nb_line=100000  # part totale extraite du fichier initial ici déjà réduit\n",
    "tauxTest = 0.05 # proportion de l'échantillon test\n",
    "data_train, data_test = split_dataset(input_path, nb_line, tauxTest)\n",
    "# Cette ligne permet de visualiser les 5 premières lignes du DataFrame \n",
    "N_train = data_train.shape[0]\n",
    "N_test = data_test.shape[0]\n",
    "print(\"Train set : %d elements, Test set : %d elements\" %(N_train, N_test))\n",
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Exploration élémentaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des catégories du premier niveau\n",
    "data_train.groupby(\"Categorie1\").first()[[\"Description\",\"Marque\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dénombrement des catégories du premier niveau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count occurence of each Categorie\n",
    "data_count = data_train[\"Categorie1\"].value_counts()\n",
    "#Rename index to add percentage\n",
    "new_index = [k+ \": %.2f%%\" %(v*100/N_train) for k,v in data_count.iteritems()]\n",
    "data_count.index= new_index\n",
    "\n",
    "fig=plt.figure(figsize= (10,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "data_count.plot.barh(logx = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que peut-on dire sur la distribution des ce classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données sont enregistrées dans les fichiers `train` et `test` de type `csv` pour des réutilisations ultérieures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.to_csv(\"data/cdiscount_test.csv\", index=False)\n",
    "data_train.to_csv(\"data/cdiscount_train_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Nettoyage des données\n",
    "Afin de limiter la dimension de l'espace des variables ou *features*, tout en conservant les informations essentielles, il est nécessaire de nettoyer les données en appliquant plusieurs étapes:\n",
    "* Chaque mot est écrit en minuscule.\n",
    "* Les termes numériques, de ponctuation et autres symboles sont supprimés.\n",
    "* 155 mots-courants, et donc non informatifs, de la langue française sont supprimés (STOPWORDS). Ex: le, la, du, alors, etc...\n",
    "* Chaque mot est \"racinisé\", via la fonction `STEMMER.stem` de la librairie nltk. La racinisation transforme un mot en son radical ou sa racine. Par exemple, les mots: cheval, chevaux, chevalier, chevalerie, chevaucher sont tous remplacés par \"cheva\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Importation des librairies et fichier pour le nettoyage des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies \n",
    "from bs4 import BeautifulSoup #Nettoyage d'HTML\n",
    "import re # Regex\n",
    "import nltk # bibliothèque de fonctions NLP\n",
    "\n",
    "## listes de mots à supprimer dans la description des produits\n",
    "## Depuis NLTK\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('french') \n",
    "## Depuis Un fichier externe.\n",
    "lucene_stopwords =open(\"data/lucene_stopwords.txt\",\"r\").read().split(\",\") #En local\n",
    "## Union des deux fichiers de stopwords \n",
    "stopwords = list(set(nltk_stopwords).union(set(lucene_stopwords)))\n",
    "\n",
    "## Fonction de stemming ou de racinisation\n",
    "stemmer=nltk.stem.SnowballStemmer('french')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Illustration sur un descriptif \n",
    "\n",
    "**Ligne Originale**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "description = data_train.Description.values[i]\n",
    "print(\"Original Description : \" + description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Supprimer les posibles balises HTML dans la description **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt = BeautifulSoup(description,\"html.parser\",from_encoding='utf-8').get_text()\n",
    "txt = BeautifulSoup(description,\"html.parser\").get_text()\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Convertir le texte en minuscule **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = txt.lower()\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Remplacer quelques caractères spéciaux **\n",
    "\n",
    "* `\\u2026`: `…`\n",
    "* `\\u00a0`: `NO-BREAK SPACE`\n",
    "\n",
    "Cette liste peut être compléter en fonction du jeu de donées étudiés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = txt.replace(u'\\u2026','.')    \n",
    "txt = txt.replace(u'\\u00a0',' ')\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Supprimer les accents **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = unicodedata.normalize('NFD', txt).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Supprimer les caractères qui ne sont ne sont pas des lettres minuscules **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = re.sub('[^a-z_]', ' ', txt)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Remplacer la description par une liste de mots (tokens), supprimer les mots de moins de 2 lettres ainsi que les stopwords **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [w for w in txt.split() if (len(w)>2) and (w not in stopwords)]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Raciniser (stem) chaque tokens **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_stem = [stemmer.stem(token) for token in tokens]\n",
    "print(tokens_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Fonctions de nettoyage de texte\n",
    "Cette fonction qui prend en entrée un texte et retourne le texte nettoyé en appliquant successivement les étapes précédentes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Fonction clean générale\n",
    "def clean_txt(txt):\n",
    "    ### remove html stuff\n",
    "    txt = BeautifulSoup(txt,\"html.parser\",from_encoding='utf-8').get_text()\n",
    "    ### lower case\n",
    "    txt = txt.lower()\n",
    "    ### special escaping character '...'\n",
    "    txt = txt.replace(u'\\u2026','.')\n",
    "    txt = txt.replace(u'\\u00a0',' ')\n",
    "    ### remove accent btw\n",
    "    txt = unicodedata.normalize('NFD', txt).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    ###txt = unidecode(txt)\n",
    "    ### remove non alphanumeric char\n",
    "    txt = re.sub('[^a-z_]', ' ', txt)\n",
    "    ### remove french stop words\n",
    "    tokens = [w for w in txt.split() if (len(w)>2) and (w not in stopwords)]\n",
    "    ### french stemming\n",
    "    tokens_stem = [stemmer.stem(token) for token in tokens]\n",
    "    ### tokens = stemmer.stemWords(tokens)\n",
    "    return ' '.join(tokens), \" \".join(tokens_stem)\n",
    "\n",
    "def clean_marque(txt):\n",
    "    txt = re.sub('[^a-zA-Z0-9]', '_', txt).lower()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "p = Pool(4)\n",
    "    \n",
    "# fonction de nettoyage du fichier(stemming et liste de mots à supprimer)\n",
    "def clean_df(input_data, column_names= ['Description', 'Libelle', 'Marque']):\n",
    "    nb_line = input_data.shape[0]\n",
    "    print(\"Start Clean %d lines\" %nb_line)\n",
    "    # Cleaning start for each columns\n",
    "    time_start = time.time()\n",
    "    clean_list=[]\n",
    "    clean_stem_list=[]\n",
    "    for column_name in column_names:\n",
    "        column = input_data[column_name].values\n",
    "        if column_name == \"Marque\":\n",
    "            array_clean = np.array(list(p.map(clean_marque,column)))\n",
    "            clean_list.append(array_clean)\n",
    "            clean_stem_list.append(array_clean)\n",
    "        else:\n",
    "            A = np.array(list(p.map(clean_txt,column)))\n",
    "            array_clean = A[:,0]\n",
    "            array_clean_stem = A[:,1]\n",
    "            clean_list.append(array_clean)\n",
    "            clean_stem_list.append(array_clean_stem)\n",
    "    time_end = time.time()\n",
    "    print(\"Cleaning time: %d secondes\"%(time_end-time_start))\n",
    "    \n",
    "    #Convert list to DataFrame\n",
    "    array_clean = np.array(clean_list).T\n",
    "    data_clean = pd.DataFrame(array_clean, columns = column_names)\n",
    "    \n",
    "    array_clean_stem = np.array(clean_stem_list).T\n",
    "    data_clean_stem = pd.DataFrame(array_clean_stem, columns = column_names)\n",
    "    return data_clean, data_clean_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Nettoyage des *data frames*\n",
    "Applique le nettoyage sur toutes les lignes du *data frame* et créé deux nouveaux *data frames*: avec et sans l'étape de racinisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take approximately 2 minutes fors 100.000 rows\n",
    "data_test_clean, data_test_clean_stem = clean_df(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_clean, data_train_clean_stem = clean_df(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affiche les 5 premières lignes du DataFrame d'apprentissage après nettoyage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_clean.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_clean_stem.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taille du dictionnaire de mots pour le dataset avant et après la racinisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_text = \" \".join(data_train_clean[\"Description\"].values)\n",
    "list_of_word = concatenate_text.split(\" \")\n",
    "N = len(set(list_of_word))\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_text = \" \".join(data_train_clean_stem[\"Description\"].values)\n",
    "list_of_word_stem = concatenate_text.split(\" \")\n",
    "N = len(set(list_of_word_stem))\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Représentations par *wordcloud*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_descr = \" \".join(data_test.Description.values)\n",
    "wordcloud_word = WordCloud(background_color=\"black\").generate_from_text(all_descr)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud_word,cmap=plt.cm.Paired)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_descr_clean_stem = \" \".join(data_test_clean_stem.Description.values)\n",
    "wordcloud_word = WordCloud(background_color=\"black\").generate_from_text(all_descr_clean_stem)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud_word,cmap=plt.cm.Paired)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarder les jeux de données nettoyés avec et sans racinisation dans des fichiers csv. Cela permet de reprendre l'analyse à la suite d'une interuption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_clean.to_csv(\"data/cdiscount_test_clean.csv\", index=False)\n",
    "data_train_clean.to_csv(\"data/cdiscount_train_clean.csv\", index=False)\n",
    "\n",
    "data_test_clean_stem.to_csv(\"data/cdiscount_test_clean_stem.csv\", index=False)\n",
    "data_train_clean_stem.to_csv(\"data/cdiscount_train_clean_stem.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\">Épisode 2</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Construction des caractéristiques ou *features* par vectorisation\n",
    "\n",
    "Les données textuelles ne peuvent pas être utilisées directment dans les différents algorithmes de d'apprentissage statistique.Plusieurs technique dites de vectorisation permettent de les traduire sous formes de vecteur numérique. Elles sont disponibles dans `Scikit-learn`:\n",
    "* `One-Hot-Encoder`\n",
    "* `Hashing`\n",
    "* `Tf-Idf`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.1 Lecture des données netoyées des étapes précédentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_clean_stem = pd.read_csv(\"data/cdiscount_test_clean_stem.csv\").fillna(\"\")\n",
    "data_train_clean_stem = pd.read_csv(\"data/cdiscount_train_clean_stem.csv\").fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dossier `features` créé ci-dessous contiendra les *data frames* des différentes possibilités de vectorisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DATA_OUTPUT_DIR = \"data/features\"\n",
    "if not(os.path.isdir(\"data/features\")):\n",
    "    os.mkdir(\"data/features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seule la colonne *Description* des `data frames` est considérée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = data_train_clean_stem[\"Description\"].values\n",
    "test_array = data_test_clean_stem[\"Description\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Vectorisation par  *One-Hot-Encoding*\n",
    "Les mots sont codés par des présences / absences. Il est également possible de prendre en compte des n-grammes. Les matrices ainsi crées sont stockés dans un format creux ou parcimonieux (*sparse*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "extr_cv = CountVectorizer(binary=False)\n",
    "data_train_OHE = extr_cv.fit_transform(train_array)\n",
    "data_train_OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = extr_cv.get_feature_names()\n",
    "N_vocabulary = len(vocabulary)\n",
    "N_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affiche la première ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = 0\n",
    "\n",
    "rw = data_train_OHE.getrow(ir)\n",
    "print(train_array[ir])\n",
    "pd.DataFrame([(v, vocabulary[v], k)  for k,v in zip(rw.data,rw.indices)], columns=[\"indices\",\"token\",\"weight\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_OHE = extr_cv.transform(test_array)\n",
    "data_test_OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = 5\n",
    "\n",
    "rw = data_test_OHE.getrow(ir)\n",
    "print(test_array[ir])\n",
    "pd.DataFrame([(v, vocabulary[v], k)  for k,v in zip(rw.data,rw.indices)], columns=[\"indices\",\"token\",\"weight\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Vectorisation par TF-IDF¶\n",
    "\n",
    "Le *TF-IDF* permet de faire ressortir l'importance relative de chaque mot $m$, ou couples de mots consécutifs (bi-gram*) dans un texte-produit ou un descriptif $d$, par rapport à la liste entière des produits ou descriptifs. La fonction $TF(m,d)$ compte le nombre d'occurences du mot $m$ dans le descriptif $d$. La fonction $IDF(m)$ mesure l'importance du terme dans l'ensemble des documents ou descriptifs en donnant plus de poids aux termes les moins fréquents car considérés comme les plus discriminants (motivation analogue à celle de la métrique du chi2 en analyse des correspondance). $IDF(m,l)=\\log\\frac{D}{f(m)}$ où $D$ est le nombre de documents, la taille de l'échantillon d'apprentissage, et $f(m)$ le nombre de documents ou descriptifs contenant le mot $m$. La nouvelle variable ou *features* est $V_m(l)=TF(m,l)\\times IDF(m,l)$.\n",
    "\n",
    "Comme pour les transformations des variables quantitatives (centrage, réduction), la même transformation c'est-à-dire les mêmes pondérations, est calculée sur l'achantillon d'apprentissage et appliquée à celui de test. La fonction `TfidfVectorizer` opèrecette transformation en produisant une matrice creuse. Fixer le paramètre `norm = False` rend les résultats plus explicite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer( ngram_range=(1,1), norm = False)\n",
    "data_train_TFIDF = vec.fit_transform(train_array)\n",
    "data_train_TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vec.get_feature_names()\n",
    "N_vocabulary = len(vocabulary)\n",
    "N_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = 0\n",
    "\n",
    "rw = data_train_TFIDF.getrow(ir)\n",
    "print(train_array[ir])\n",
    "pd.DataFrame([(v, vocabulary[v], vec.idf_[v], k)  for k,v in zip(rw.data,rw.indices)], columns=[\"indices\",\"token\",\"idf\",\"weight\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `TfidfVectorizer` admet plusieurs variantes: *smooth idf, sublinear_tf* ainsi que la possibilité de prendre en compte des n-gram mais la taille du dictionnaire explose.\n",
    "\n",
    "La même transformation de vectorisation est ensuite appliquée sur le jeu de données test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_TFIDF = vec.transform(test_array)\n",
    "data_test_TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le *tf* est évalué pour chaque ligne ou descriptif mais pondéré par le même *idf*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = 5\n",
    "\n",
    "rw = data_test_TFIDF.getrow(ir)\n",
    "print(test_array[ir])\n",
    "pd.DataFrame([(v, vocabulary[v], vec.idf_[v], k)  for k,v in zip(rw.data,rw.indices)], columns=[\"indices\",\"token\",\"idf\",\"weight\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Fonction de Hashage\n",
    "\n",
    "Le *hashnig* permet de réduire l'espace des variables (taille du dictionnaire) en un nombre limité et fixé a priori `n_hash` de caractéristiques ou *features*. Il repose sur la définition d'une fonction de hashage $h$ qui à un indice $j$ défini dans l'espace des entiers naturels, renvoie de façon déterministe et injective un indice $i=h(j)$ dans dans l'espace réduit (1 à n_hash) des caractéristiques. Ainsi le poids de l'indice $i$, du nouvel espace, est l'association de tous les poids d'indice $j$ tels que $i=h(j)$ de l'espace originale. Ici, les poids sont associés d'après la méthode décrite par Weinberger et al. (2009).\n",
    "\n",
    "N.B. $h$ n'est pas généré aléatoirement. Ainsi pour un même fichier d'apprentissage (ou de test) et pour un même entier `n_hash`, le résultat de la fonction de hashage est identique\n",
    "\n",
    "Le dictionnaire est pris en paramètre d'entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "nb_hash = 300\n",
    "\n",
    "feathash = FeatureHasher(nb_hash)\n",
    "train_dict_array  = map(lambda x : collections.Counter(x.split(\" \")), train_array)\n",
    "data_train_hash = feathash.fit_transform(train_dict_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = 0\n",
    "\n",
    "rw = data_train_hash.getrow(ir)\n",
    "print(train_array[ir])\n",
    "pd.DataFrame([(v, k)  for k,v in zip(rw.data,rw.indices)], columns=[\"indices\",\"weight\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction réduit la taille de la matrice, mais il n'existe pas de fonction inverse explicite à cette transformation. Le résultat perd en lisibilit ou interprétabilité. C'est néanmoins indispensbale si le dictionnaire est très volumineux. Pour la partie d'apprentissage, il est possible d'utiliser la matrice de hashage obtenue comme des présence absence ou encore de lui associer les pondérations issues d'une transformation TF-IDF comme ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vec =  TfidfTransformer(norm = False)\n",
    "data_train_HTfidf = vec.fit_transform(data_train_hash)\n",
    "data_train_HTfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = 0\n",
    "\n",
    "rw = data_train_HTfidf.getrow(ir)\n",
    "print(train_array[ir])\n",
    "pd.DataFrame([(v, vec.idf_[v], k)  for k,v in zip(rw.data, rw.indices)], columns=[\"indices\",\"idf_\",\"weight\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Fonctions de Vectorisation\n",
    "\n",
    "Ces transformaitons asdmettent de nombreuses options dont il faut fixer les valeurs des paramètres. Dans les présents tests, seuls 4 jeux d'apprentissage et test sont construits:\n",
    "* `count` avec des occurences (*one hot*) de 0,1 \n",
    "   - sans hashage \n",
    "   - avec `n_hash=300`\n",
    "* `TFIDF` \n",
    "   - sans hashage \n",
    "   - avec `n_hash=300`\n",
    "\n",
    "D'autres options pourraient être considérées en introduisant des n-grams ou en faisant varier `n_hash` ou encore les variantes de *TFIDF*. \n",
    "\n",
    "Les deux fonctions `vectorizer_train` and `apply_vectorizer` permettent de générer les différentes options sur l'échantillon d'apprentissage puis de les appliquéer également à l'échantillon test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def vectorizer_train(df, columns=['Description'], nb_hash=None, nb_gram = 1, vectorizer = \"tfidf\" , binary = False):\n",
    "    \n",
    "    data_array = [\" \".join(line) for line in df[columns].values]\n",
    "    \n",
    "    # Hashage\n",
    "    if nb_hash is None:\n",
    "        feathash = None\n",
    "        if vectorizer == \"tfidf\":\n",
    "            vec = TfidfVectorizer(ngram_range=(1,nb_gram))\n",
    "            data_vec = vec.fit_transform(data_array)\n",
    "        else:\n",
    "            vec = CountVectorizer(binary=binary)\n",
    "            data_vec = vec.fit_transform(data_array)\n",
    "    else:\n",
    "        data_dic_array = [collections.Counter(line.split(\" \")) for line in data_array]\n",
    "        feathash = FeatureHasher(nb_hash)\n",
    "        data_hash = feathash.fit_transform(data_dic_array)\n",
    "        \n",
    "        if vectorizer==\"tfidf\":\n",
    "            vec =  TfidfTransformer()\n",
    "            data_vec =  vec.fit_transform(data_hash)\n",
    "        else:\n",
    "            vec = None\n",
    "            data_vec = data_hash\n",
    "\n",
    "    return vec, feathash, data_vec\n",
    "\n",
    "\n",
    "\n",
    "def apply_vectorizer(df, vec, feathash, columns =['Description', 'Libelle', 'Marque']):\n",
    "    \n",
    "    data_array = [\" \".join(line) for line in df[columns].values]\n",
    "    \n",
    "    #Hashage\n",
    "    if feathash is None:\n",
    "        data_hash = data_array\n",
    "    else:\n",
    "        data_dic_array = [collections.Counter(line.split(\" \")) for line in data_array]\n",
    "        data_hash = feathash.transform(data_dic_array)\n",
    "    \n",
    "    if vec is None:\n",
    "        data_vec = data_hash\n",
    "    else:\n",
    "        data_vec = vec.transform(data_hash)\n",
    "    return data_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création des couples de données apprentissage x test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [[None, \"count\"],\n",
    "              [300, \"count\"],\n",
    "              [10000,\"count\"],\n",
    "              [None, \"tfidf\"],\n",
    "              [300, \"tfidf\"],\n",
    "             [10000,\"tfidf\"]]\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "for nb_hash, vectorizer in parameters:\n",
    "    ts = time.time()\n",
    "    vec, feathash, data_train_vec = vectorizer_train(data_train_clean_stem, nb_hash=nb_hash, vectorizer = vectorizer)\n",
    "    data_test_vec = apply_vectorizer(data_test_clean_stem, vec, feathash)\n",
    "    te = time.time()\n",
    "    \n",
    "    print(\"nb_hash : \" + str(nb_hash) + \", vectorizer : \" + str(vectorizer))\n",
    "    print(\"Runing time for vectorization : %.1f seconds\" %(te-ts))\n",
    "    print(\"Train shape : \" + str(data_train_vec.shape))\n",
    "    print(\"Test shape : \" + str(data_test_vec.shape))\n",
    "\n",
    "    \n",
    "    sparse.save_npz(DATA_OUTPUT_DIR +\"/vec_train_nb_hash_\" + str(nb_hash) + \"_vectorizer_\" + str(vectorizer), data_train_vec)\n",
    "    sparse.save_npz(DATA_OUTPUT_DIR +\"/vec_test_nb_hash_\" + str(nb_hash) + \"_vectorizer_\" + str(vectorizer), data_test_vec)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\">Épisode 3</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Prévision de la catégorie d'un texte.\n",
    "Les deux vectorisations sont utilisées avec différentes valeurs de `n_hash` pour entraîner un algorithme d'apprentissage afin de prévoir les catégories de l'échantillon test puis comparer les erreurs de prévision. \n",
    "\n",
    "Deux algorihtmes sont utilisés: la régression logistique et les forêts aléatoires.\n",
    "\n",
    "**Attention** les temps d'exécution sont assez long..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "Y_train = pd.read_csv(\"data/cdiscount_train_subset.csv\").fillna(\"\")[\"Categorie1\"]\n",
    "Y_test = pd.read_csv(\"data/cdiscount_test.csv\").fillna(\"\")[\"Categorie1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/features\"\n",
    "from scipy import sparse\n",
    "metadata_list_lr = []\n",
    "parameters = [[None, \"count\"],\n",
    "              [300, \"count\"],\n",
    "              [10000, \"count\"],\n",
    "              [None, \"tfidf\"],\n",
    "              [300, \"tfidf\"],\n",
    "              [10000, \"tfidf\"],]\n",
    "\n",
    "for nb_hash, vectorizer in parameters:\n",
    "    print(\"nb_hash : \" + str(nb_hash) + \", vectorizer : \" + str(vectorizer))\n",
    "    X_train = sparse.load_npz(DATA_DIR +\"/vec_train_nb_hash_\" + str(nb_hash) + \"_vectorizer_\" + str(vectorizer)+\".npz\")\n",
    "    X_test = sparse.load_npz(DATA_DIR +\"/vec_test_nb_hash_\" + str(nb_hash) + \"_vectorizer_\" + str(vectorizer)+\".npz\")\n",
    "    ts = time.time()\n",
    "    cla = LogisticRegression(n_jobs=-1)\n",
    "    cla.fit(X_train,Y_train.values)\n",
    "    te=time.time()\n",
    "    t_learning = te-ts\n",
    "    ts = time.time()\n",
    "    score_train=cla.score(X_train,Y_train)\n",
    "    score_test=cla.score(X_test,Y_test)\n",
    "    te=time.time()\n",
    "    t_predict = te-ts\n",
    "    metadata = {\"typeW2V\": None, \"nb_hash\": nb_hash, \"vectorizer\":vectorizer , \"learning_time\" : t_learning, \"predict_time\":t_predict, \"score_train\": score_train, \"score_test\": score_test}\n",
    "    print(metadata)\n",
    "    metadata_list_lr.append(metadata)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Forêt aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "metadata_list_rf = []\n",
    "\n",
    "parameters = [[None, \"count\"],\n",
    "              [300, \"count\"],\n",
    "              [10000, \"count\"],\n",
    "              [None, \"tfidf\"],\n",
    "              [300, \"tfidf\"],\n",
    "              [10000, \"tfidf\"],]\n",
    "\n",
    "for nb_hash, vectorizer in parameters:\n",
    "    print(\"nb_hash : \" + str(nb_hash) + \", vectorizer : \" + str(vectorizer))\n",
    "    X_train = sparse.load_npz(DATA_DIR +\"/vec_train_nb_hash_\" + str(nb_hash) + \"_vectorizer_\" + str(vectorizer)+\".npz\")\n",
    "    X_test = sparse.load_npz(DATA_DIR +\"/vec_test_nb_hash_\" + str(nb_hash) + \"_vectorizer_\" + str(vectorizer)+\".npz\")\n",
    "    ts = time.time()\n",
    "    cla = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "    cla.fit(X_train,Y_train.values)\n",
    "    te=time.time()\n",
    "    t_learning = te-ts\n",
    "    ts = time.time()\n",
    "    score_train=cla.score(X_train,Y_train)\n",
    "    score_test=cla.score(X_test,Y_test)\n",
    "    te=time.time()\n",
    "    t_predict = te-ts\n",
    "    metadata = {\"typeW2V\": None, \"nb_hash\": nb_hash, \"vectorizer\":vectorizer , \"learning_time\" : t_learning, \"predict_time\":t_predict, \"score_train\": score_train, \"score_test\": score_test}\n",
    "    print(metadata)\n",
    "    metadata_list_rf.append(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparaisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(metadata_list_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(metadata_list_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\">Épisode 4</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Vectorisation par  `Word2Vec`\n",
    "Cette fonction consiste à opérer un *plongement* (*embedding*) du dictionnaire dans un espace vectoriel de dimension fixé. Chaque mot est alors un vecteur de cet espace dont grosso-modo la proximité au sens de la métrique euclidienne est fonction des co-occurences des mots dans un proche voisinage au sein d'un même texte. Le voisinnage est défini par une fenêtre de taille ou nombre de mots consécutifs fixé. \n",
    "\n",
    "Cette fonction est techniquement l'application d'un algorithme *autoencoder-decoder* à trois couches. La couche d'entrée est celle de sortie ont la taille du dictionnaire, celle cachée a pour taille la dimension de l'espace vectoriel. Deux options sont disponibles `CBOW` ( continuous bag-of-words) ou `skip-gram` selon que le mot est entrée associé en sortie avec les mots présents dans la fenètre ou *vice-versa*. \n",
    "\n",
    "La fonction `word2vec` intègre à sa façon l'environnement de chaque mot et donc une part séantique de chaque texte mais san sprende en compte l'ordre des mots.\n",
    "\n",
    "De très nombreuses astuces algorithmiques permettent d'évaluer cette matrice de vecteurs (un par mot) même sur des corpus de donées très volumineux comme les *google news* ou les pages Wikipedia dans une langue déterminée. \n",
    "\n",
    "Il est ainsi possible d'apprendre un \"modèle\" à partir de son corpus de données ou encore de charger un module déjà appris. [Kyubyong Park](https://github.com/Kyubyong/wordvectors) propose des modèles pour une trentaine de langues dont le français.\n",
    "\n",
    "La librairie `gensim` opère ce *Word Embedding* par `Word2Vec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "import nltk \n",
    "stemmer=nltk.stem.SnowballStemmer('french')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Construire un modèle  Word2Vec\n",
    "Préparation des données et choix des options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array_token = [line.split(\" \") for line in train_array]\n",
    "test_array_token = [line.split(\" \") for line in test_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choix des paramètres ci-dessous:\n",
    "\n",
    "La valeur de `sg` détermine l'option: \"1\" pour `skip-gram` ou \"0\" pour `CBOW`. \n",
    "\n",
    "`hs` dirige les options de l'algorihtme. \n",
    "* `hs=1` l'entraînement est opéré avec une fonction de `softmax` hiérarchique\n",
    "* `hs=0` et `negative` entier entre 5 et 20. Spécifie combien de \"nots bruits\" sont tirés aléatoirement parmi ceux qui ne sont pas dans la fenêtre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_dimension = 300\n",
    "sg = 1\n",
    "hs = 0\n",
    "negative = 10\n",
    "X = train_array_token\n",
    "N_train = len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimation des modèles à partir du corpus de textes pour les deux modes de l'algorihtme: `skip-gram`  ou `CBOW`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dic = {}\n",
    "for sg in [0,1]:\n",
    "    print(\"Start learning Word2Vec learning\")\n",
    "    print(\"Params are : Fdim_%d_sg_%d_hs_%d_negative_%d_model\" %(Features_dimension, sg, hs, negative))\n",
    "    ts = time.time()\n",
    "    model = gensim.models.Word2Vec(X, sg=sg, hs=hs, negative=negative, min_count=1, size=Features_dimension)\n",
    "    te = time.time()\n",
    "    t_learning = te-ts\n",
    "\n",
    "    # Metadata\n",
    "    N_vocab, feature_dim = model.wv.vectors.shape\n",
    "    metadata = {\"learning_time\" : t_learning, \"vocab_size\" : N_vocab, \"sg\" : sg, \"negative\": negative, \"hs\":hs}\n",
    "    print(metadata)\n",
    "    \n",
    "    model_name = \"skip-gram\" if sg==1 else \"CBOW\"\n",
    "    model_dic.update({model_name : model})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charger puis décompresser le modèle de  [Kyubyong Park](https://github.com/Kyubyong/wordvectors) pour le français. dans le répertoire `data` avant de charger ce modèle `on line`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_online_dir = \"data/fr/fr.bin\"\n",
    "model = gensim.models.Word2Vec.load(model_online_dir)\n",
    "model_dic.update({\"online\" : model})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Utilisations de Word2Vec en NLP\n",
    "Cette section illustre quelques unes des fonctions classiques d'un modèle Word2Vec avant de revenir à l'objectif de classificaiton supervisée.\n",
    "#### Mots les plus similaires\n",
    "Les résultats sont pertinent avec un volume suffisant; *e.g.* `N_train=1.000.000`; l'effectif de 100.000 est trop réduit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_output_word([\"homm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term=\"homme\"\n",
    "\n",
    "df_ = []\n",
    "columns = []\n",
    "for model_name, model in model_dic.items():\n",
    "    token = stemmer.stem(term) if \"online\"!=model_name else term\n",
    "    mpow = model.wv.most_similar([token])\n",
    "    if mpow is None:\n",
    "        df_.append([\"\" for k in range(10)])\n",
    "    else:\n",
    "        df_.append([k[0] for k in mpow])\n",
    "    columns.append(model_name)\n",
    "print(\"Most similar words for word : \"+term)\n",
    "pd.DataFrame(np.array(df_).T, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term=\"femme\"\n",
    "\n",
    "df_ = []\n",
    "columns = []\n",
    "for model_name, model in model_dic.items():\n",
    "    token = stemmer.stem(term) if \"online\"!=model_name else term\n",
    "    mpow = model.wv.most_similar([token])\n",
    "    if mpow is None:\n",
    "        df_.append([\"\" for k in range(10)])\n",
    "    else:\n",
    "        df_.append([k[0] for k in mpow])\n",
    "    columns.append(model_name)\n",
    "print(\"Most similar words for word : \"+term)\n",
    "pd.DataFrame(np.array(df_).T, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term=\"xbox\"\n",
    "\n",
    "df_ = []\n",
    "columns = []\n",
    "for model_name, model in model_dic.items():\n",
    "    token = stemmer.stem(term) if \"online\"!=model_name else term\n",
    "    mpow = model.wv.most_similar([token])\n",
    "    if mpow is None:\n",
    "        df_.append([\"\" for k in range(10)])\n",
    "    else:\n",
    "        df_.append([k[0] for k in mpow])\n",
    "    columns.append(model_name)\n",
    "print(\"Most similar words for word : \"+term)\n",
    "pd.DataFrame(np.array(df_).T, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combinaison de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_positif = [\"femme\",\"roi\"]\n",
    "terms_negatif = [\"homme\"]\n",
    "\n",
    "df_ = []\n",
    "columns = []\n",
    "for model_name, model in model_dic.items():\n",
    "    token_positif = [stemmer.stem(term) if \"online\"!=model_name else term for term in terms_positif]\n",
    "    token_negativ = [stemmer.stem(term) if \"online\"!=model_name else term for term in terms_negatif]\n",
    "    \n",
    "    mpow = model.wv.most_similar(positive=token_positif, negative=token_negativ)\n",
    "    if mpow is None:\n",
    "        df_.append([\"\" for k in range(10)])\n",
    "    else:\n",
    "        df_.append([k[0] for k in mpow])\n",
    "    columns.append(model_name)\n",
    "pd.DataFrame(np.array(df_).T, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_positif = [\"espagne\",\"paris\"]\n",
    "terms_negatif = [\"france\"]\n",
    "\n",
    "df_ = []\n",
    "columns = []\n",
    "for model_name, model in model_dic.items():\n",
    "    token_positif = [stemmer.stem(term) if \"online\"!=model_name else term for term in terms_positif]\n",
    "    token_negativ = [stemmer.stem(term) if \"online\"!=model_name else term for term in terms_negatif]\n",
    "    \n",
    "    mpow = model.wv.most_similar(positive=token_positif, negative=token_negativ)\n",
    "    if mpow is None:\n",
    "        df_.append([\"\" for k in range(10)])\n",
    "    else:\n",
    "        df_.append([k[0] for k in mpow])\n",
    "    columns.append(model_name)\n",
    "pd.DataFrame(np.array(df_).T, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compléter une séquence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [\"voir\",\"la\"]\n",
    "\n",
    "\n",
    "df_ = []\n",
    "columns = []\n",
    "for model_name, model in model_dic.items():\n",
    "    tokens = [stemmer.stem(term) if \"online\"!=model_name else term for term in terms]\n",
    "    \n",
    "    mpow = model.predict_output_word(tokens)\n",
    "    if mpow is None:\n",
    "        df_.append([\"\" for k in range(10)])\n",
    "    else:\n",
    "        df_.append([k[0] for k in mpow])\n",
    "    columns.append(model_name)\n",
    "pd.DataFrame(np.array(df_).T, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [\"coque\",'pour',\"samsung\"]\n",
    "\n",
    "\n",
    "df_ = []\n",
    "columns = []\n",
    "for model_name, model in model_dic.items():\n",
    "    tokens = [stemmer.stem(term) if \"online\"!=model_name else term for term in terms]\n",
    "    \n",
    "    mpow = model.predict_output_word(tokens)\n",
    "    if mpow is None:\n",
    "        df_.append([\"\" for k in range(10)])\n",
    "    else:\n",
    "        df_.append([k[0] for k in mpow])\n",
    "    columns.append(model_name)\n",
    "pd.DataFrame(np.array(df_).T, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [\"homme\"]\n",
    "\n",
    "\n",
    "df_ = []\n",
    "columns = []\n",
    "for model_name, model in model_dic.items():\n",
    "    tokens = [stemmer.stem(term) if \"online\"!=model_name else term for term in terms]\n",
    "    \n",
    "    mpow = model.predict_output_word(tokens)\n",
    "    if mpow is None:\n",
    "        df_.append([\"\" for k in range(10)])\n",
    "    else:\n",
    "        df_.append([k[0] for k in mpow])\n",
    "    columns.append(model_name)\n",
    "pd.DataFrame(np.array(df_).T, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [\"femme\"]\n",
    "\n",
    "\n",
    "df_ = []\n",
    "columns = []\n",
    "for model_name, model in model_dic.items():\n",
    "    tokens = [stemmer.stem(term) if \"online\"!=model_name else term for term in terms]\n",
    "    \n",
    "    mpow = model.predict_output_word(tokens)\n",
    "    if mpow is None:\n",
    "        df_.append([\"\" for k in range(10)])\n",
    "    else:\n",
    "        df_.append([k[0] for k in mpow])\n",
    "    columns.append(model_name)\n",
    "pd.DataFrame(np.array(df_).T, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Construire des *features* avec *Word2Vec*\n",
    "Un texte ou descriptif d'article est représenté par le barycentre des mots qui le composent. Ce barycentre peut éventuellement être calculé avec des pondérations. Le nombre de caractéristiques ou variables décrivant chaque texte est la dimension de l'espace des mots, ici 300. C'est donc une façon de prendre en compte le contexte des mots tout en réduisant drastiquement la dimension de l'espace.\n",
    "\n",
    "#### Avec les modèles appris\n",
    "Prise ne compte des textes puis définition des fonctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_clean = pd.read_csv(\"data/cdiscount_test_clean.csv\").fillna(\"\")\n",
    "data_train_clean = pd.read_csv(\"data/cdiscount_train_clean.csv\").fillna(\"\")\n",
    "\n",
    "train_array_token_wstem = [line.split(\" \") for line in data_train_clean[\"Description\"].values]\n",
    "test_array_token_wstem = [line.split(\" \") for line in data_test_clean[\"Description\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_mean(lines):\n",
    "    features = [MODEL[x] for x  in lines if x in MODEL]\n",
    "    if features == []:   \n",
    "        fm =np.ones(F_SIZE)\n",
    "    else :\n",
    "        fm = np.mean(features,axis=0)\n",
    "    return fm\n",
    "\n",
    "def get_matrix_features_means(X):\n",
    "    X_embedded_ = list(map(get_features_mean, X))\n",
    "    X_embedded = np.vstack(X_embedded_)\n",
    "    return X_embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul de la matrice des caractéristiques ou *features* pour les trois modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in [\"CBOW\",\"skip-gram\", \"online\"]:\n",
    "    \n",
    "    if \"online\" == model_name:\n",
    "        X_train = train_array_token_wstem\n",
    "        X_test = test_array_token_wstem\n",
    "    else:\n",
    "        X_train = train_array_token\n",
    "        X_test = test_array_token\n",
    "    \n",
    "    model = model_dic[model_name]\n",
    "    MODEL = model\n",
    "    F_SIZE = Features_dimension\n",
    "\n",
    "    ts = time.time()\n",
    "    X_embedded_train = get_matrix_features_means(X_train)\n",
    "    te = time.time()\n",
    "    t_train = te-ts\n",
    "    #np.save(embedded_train_dir, X_embedded_train)\n",
    "    print(\"Time conversion : %d seconds\"%t_train)\n",
    "    print(\"Shape Matrix : (%d,%d)\"%X_embedded_train.shape)\n",
    "    np.save(DATA_OUTPUT_DIR +\"/embedded_train_nb_hash_\"+model_name, X_embedded_train)\n",
    "\n",
    "    ts = time.time()\n",
    "    X_embedded_test = get_matrix_features_means(X_test)\n",
    "    te = time.time()\n",
    "    t_test = te-ts\n",
    "    #np.save(embedded_test_dir, X_embedded_test)\n",
    "    print(\"Time conversion : %d seconds\"%t_test)\n",
    "    print(\"Shape Matrix : (%d,%d)\"%X_embedded_test.shape)\n",
    "    np.save(DATA_OUTPUT_DIR +\"/embedded_test_nb_hash_\"+model_name, X_embedded_test)\n",
    "    \n",
    "    metadata = {\"t_train\" : t_train, \"t_test\" : t_test, \"sg\":sg}\n",
    "    print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[\"homme\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.syn0[model.wv.vocab[\"homme\"].index] == model[\"homme\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.syn1neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vh = model.wv.vocab[\"homme\"]\n",
    "vh.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\">Épisode 5</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Prévision de la catégorie d'un texte.\n",
    "La vectorisation issue de Word2Vec est utilisée pour entraîner un algorithme d'apprentissage afin de prévoir les catégories de l'échantillon test et mesurer puis comparer les erreurs de prévision obtenues à la suite des vectorisation précédentes.\n",
    "\n",
    "Deux algorihtmes sont utilisés: la régression logistique et les forêts aléatoires.\n",
    "\n",
    "**Attention** les temps d'exécution sont assez long...\n",
    "\n",
    "Fixer le paramètre `njobs=-1` permet d'activer tous les processeurs. Le gain de temps est très sensible pour les forêts aléatoires mais cette valeur n'est pas permise pour le solveur utilisé par défaut en réression logistique. Il faudrait sans doute changer cette dernière option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = pd.read_csv(\"data/cdiscount_train_subset.csv\").fillna(\"\")[\"Categorie1\"]\n",
    "Y_test = pd.read_csv(\"data/cdiscount_test.csv\").fillna(\"\")[\"Categorie1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Régression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model_name in [\"CBOW\",\"skip-gram\", \"online\"]:\n",
    "    print(\"Word2Vec :\" + model_name)\n",
    "\n",
    "    X_train = np.load(DATA_DIR +\"/embedded_train_nb_hash_\" + model_name+\".npy\")\n",
    "    X_test = np.load(DATA_DIR +\"/embedded_test_nb_hash_\" + model_name+\".npy\")\n",
    "    \n",
    "    ts = time.time()\n",
    "    cla = LogisticRegression()\n",
    "    cla.fit(X_train,Y_train.values)\n",
    "    te=time.time()\n",
    "    t_learning = te-ts\n",
    "    ts = time.time()\n",
    "    score_train=cla.score(X_train,Y_train)\n",
    "    score_test=cla.score(X_test,Y_test)\n",
    "    te=time.time()\n",
    "    t_predict = te-ts\n",
    "    metadata = {\"typeW2V\": model_name ,\"nb_hash\": None, \"vectorizer\":\"word2vec\" ,\"learning_time\" : t_learning, \"predict_time\":t_predict, \"score_train\": score_train, \"score_test\": score_test}\n",
    "    print(metadata)\n",
    "    metadata_list_lr.append(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(\"data/features/embedded_train_nb_hash_CBOW.npy\").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparaison des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(metadata_list_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Forêt aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in [\"CBOW\",\"skip-gram\", \"online\"]:\n",
    "    print(\"Word2Vec :\" + model_name)\n",
    "\n",
    "    X_train = np.load(DATA_DIR +\"/embedded_train_nb_hash_\" + model_name+\".npy\")\n",
    "    X_test = np.load(DATA_DIR +\"/embedded_test_nb_hash_\" + model_name+\".npy\")\n",
    "    \n",
    "    ts = time.time()\n",
    "    cla = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "    cla.fit(X_train,Y_train.values)\n",
    "    te=time.time()\n",
    "    t_learning = te-ts\n",
    "    ts = time.time()\n",
    "    score_train=cla.score(X_train,Y_train)\n",
    "    score_test=cla.score(X_test,Y_test)\n",
    "    te=time.time()\n",
    "    t_predict = te-ts\n",
    "    metadata = {\"typeW2V\": model_name ,\"nb_hash\": None, \"vectorizer\":\"word2vec\" ,\"learning_time\" : t_learning, \"predict_time\":t_predict, \"score_train\": score_train, \"score_test\": score_test}\n",
    "    print(metadata)\n",
    "    metadata_list_rf.append(metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparaison des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(metadata_list_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion ?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {
    "height": "279px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
