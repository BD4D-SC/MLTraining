{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Machine Learning for Data Science, CERFACS, May 2019\n",
    "\n",
    "## Introduction to bandit problems and algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import *\n",
    "from math import *\n",
    "from numpy import *\n",
    "from numpy.random import *\n",
    "from scipy.misc import *\n",
    "from scipy.stats import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sequel we consider the stochastic bandit problem with only two actions $0$ and $1$. We assume that their rewards are Bernoulli random variables with parameters $\\mu_0 = 0.6$ and $\\mu_1 = 0.4$ (so that action $0$ is optimal, but we have to learn it from experience!).\n",
    "\n",
    "### Simulation of Bernoulli random variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial(n=10,p=0.5,size=46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial(n=1,p=0.5,size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation:\n",
    "- The arm played by the algorithm at time $t$ is denoted by $I_t$.\n",
    "- The reward of arm $a$ at time $t$ is $g_t(a)$.\n",
    "- We write $\\hat{\\mu}_t(a)$ for the average of all the rewards obtained when playing arm $a \\in \\{0,1\\}$ from iteration $0$ up to iteration $t$.\n",
    "- We also write $T_a(t)$ for the number of times arm $a$ was played up to iteration $t$.\n",
    "- In particular, we have\n",
    "$$\\hat{\\mu}_t(a) = \\frac{1}{T_a(t)} \\sum_{k=1}^t g_k(a) \\mathbb{1}_{I_k = a}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why \"Follow the best arm\" is a bad idea.\n",
    "\n",
    "The \"Follow the best arm\" algorithm proceeds as follows:\n",
    "- alternatively play actions $0$ and $1$ from iteration $t=0$ up to iteration $t=\\tau$\n",
    "- determine the empirical best arm\n",
    "$$A \\in \\rm{argmin}_{a \\in \\{0,1\\}} \\hat{\\mu}_{\\tau}(a)$$\n",
    "- play arm A forever.\n",
    "\n",
    "Show that, if $\\tau$ is fixed, then the expected regret after $T$ rounds grows linearly with $T$ when $T \\to +\\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The $\\epsilon$-greedy algorithm.\n",
    "\n",
    "The \"$\\epsilon$-greedy algorithm\" proceeds as follows:\n",
    "- For the first two iterations: play actions $0$ and $1$ once\n",
    "- At each iteration $t=2,3,4,\\ldots$,\n",
    "    - with probability $1-\\epsilon_t$, play the empirical best arm\n",
    "    - with probability $\\epsilon_t$, play action $0$ or $1$ uniformly at random\n",
    "   \n",
    "Show that, if $\\epsilon_t = \\frac{c}{t}$ with a large enough constant $c$, then the expected regret after $T$ rounds only grows logarithmically when $T \\to +\\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The UCB algorithm.\n",
    "\n",
    "The UCB-algorithm proceeds as follows:\n",
    "- For the first two iterations: play actions $0$ and $1$ once\n",
    "- At each iteration $t=2,3,4,\\ldots$, play the arm that maximizes the Upper Confidence Bound:\n",
    "$$I_t \\in \\rm{argmax}_{a \\in \\{0,1\\}} \\left\\{\\hat{\\mu}_{t-1}(a) + \\sqrt{\\frac{2 \\log(t)}{T_a(t-1)}} \\;\\right\\}$$\n",
    "   \n",
    "Show that the expected regret after $T$ rounds also grows logarithmically when $T \\to +\\infty$. Compare with the first two algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
